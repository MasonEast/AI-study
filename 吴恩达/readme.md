# 学习算法

监督学习
无监督学习
强化学习

## 监督学习

输入到输出价值

关键特征：提供学习案例，每个案例都有正确的答案。给定输入 x 的正确标签 y，学习算法的目标是找到函数 f，使得 f(x) 接近 y。

主要算法有：线性回归，分类算法

学习算法必须决定如何将边界线拟合到这些数据上

### 线性回归

从无数的数值中，找到一条直线（曲线），使得这条线与所有点的距离之和最小，从而预测一个最接近的数值。
比如：预测房价

#### 矩阵

X = [[x11, x12],
     [x21, x22],
     [x31, x32]]

一个二维矩阵 x，形状是（m, n），每一行是一个样本，m 是行数，代表样本数量，每一列是一种特征，n 是列数，代表特征数量。

x.shape[0] 代表行数，x.shape[1] 代表列数。

theta 是一个一维向量，形状为 (n,)，包含线性回归模型的参数（或权重）。theta 的每个元素对应于 X 中每个特征的权重（系数）。

#### 预测值：X @ theta

预测值就是每个样本特征与特征权重的乘积之和。

X @ theta 是一个矩阵乘法操作，它将 X 和 theta 相乘，得到一个新的矩阵。这个新矩阵的形状是 (m, 1)，其中 m 是 X 的行数。这个新矩阵的每一行都是 X 的每一行与 theta 的乘积，即每个样本的预测值。

X @ theta 的计算过程为：

```python
X @ theta = [x11 * theta1 + x12 * theta2,
            x21 * theta1 + x22 * theta2,
            x31 * theta1 + x32 * theta2]
```

这就得到一个形状为 (3,) 的向量，其中每个元素是对应样本的预测值。

X @ theta 实际上计算了通过线性组合特征来得到每个样本的预测输出。线性组合是通过将每个特征值乘以其对应的权重（参数）然后求和来实现的。

#### 损失函数（代价函数）

损失函数（Cost Function）是衡量模型预测值与实际值之间差距的函数。在线性回归中，常用的损失函数是均方误差（Mean Squared Error, MSE）。

```python
theta = np.zeros(X.shape[1])

def lr_cost(theta, X, y):
    m = X.shape[0] # 样本数量
    h = X @ theta # 预测值
    inner = h - y # 预测值与真实值之间的差值
    squared_error = inner.T @ inner # 差值的平方
    J = squared_error / (2 * m) # 把误差平方和除以样本数量的两倍来计算。除以 2*m 是一种约定，在使用梯度下降时能够简化代价函数的梯度计算。
    return J
```

#### 梯度下降法

梯度下降法是一种优化算法，用于最小化损失函数。在机器学习中，我们通常使用梯度下降法来找到模型的参数，使得模型的预测结果与实际结果之间的差距最小。

梯度下降法的核心思想是沿着损失函数的梯度方向不断更新参数，从而逐步逼近最优解。具体来说，梯度下降法通过以下步骤来更新参数：

1. 初始化参数：首先需要选择一个初始的参数值，这个值可以是随机的，也可以是零向量。
2. 计算梯度：计算损失函数关于参数的梯度，即损失函数的偏导数。梯度表示了损失函数在当前参数值下的斜率，指向了损失函数增长最快的方向。
3. 更新参数：根据梯度和学习率（步长）来更新参数。更新公式为：θ = θ - α \* ∇J(θ)，其中 α 是学习率，∇J(θ) 是梯度。
4. 重复步骤 2 和步骤 3：重复计算梯度并更新参数，直到达到停止条件，比如达到最大迭代次数或者损失函数的值变化小于某个阈值。

```python
def gradient_descent(X, y, theta, alpha, num_iters):
    m = len(y) # 样本数量
    J_history = np.zeros(num_iters) # 存储每次迭代的损失函数值

    for i in range(num_iters):
        h = X @ theta # 预测值
```

##### 检查梯度下降是否收敛

#### 特征缩放

当特征具有不同的量纲时，梯度下降法可能会收敛得很慢。为了解决这个问题，我们需要对特征进行缩放，使得它们具有相同的量纲。常见的特征缩放方法包括标准化和归一化。

比如： 预测房子价格，房子面积会很大，但是房间数量很小，我们需要对特征进行缩放，使得它们具有相同的量纲。

方法：归一化，将特征值缩放到 [0, 1] 之间。具体公式为：x_scaled = (x - min(x)) / (max(x) - min(x))。

### 分类算法

与回归不同，这里可能只预测一小部分可能的输出，比如 0 或 1，或者一个类别。回归试图预测无限多个可能数字中的一个
比如：判断肿瘤是否良性，垃圾邮件过滤

### 逻辑回归

#### 损失函数

#### 决策边界

#### 多元分类

#### 过拟合

#### 正则化

### 神经网络

#### 神经元

#### 激活函数

#### 前向传播

#### 反向传播

#### 深度学习

## 无监督学习

在监督学习中，我们有一个输入 x 和一个输出标签 y，并且我们知道正确的答案是什么。
而无监督学习中，仅有输入 x 而没有输出标签 y，没有正确的答案，算法试图从数据中学习一些结构，从中提炼有用的信息

主要算法有：聚类，降维

### 聚类

将无标签数据自动分成不同的组，每个组包含相似的标签

比如：基因测序，公司客户细分，新闻推荐系统

### 降维

将一个大数据集压成一个小数据集，同时丢失尽可能少的信息，保留数据中的大部分信息。

### 异常检测

## 强化学习

## Jupyter Notebook

## 术语

- 数据集 dataset：用于训练模型的数据
- 特征（输入特征）feature：数据集中的输入变量 x
- 标签（输出标签）output variable，target variable：数据集中的输出变量 y
- 训练总数 number of training examples：数据集中训练样本的数量 m
- 训练示例 training example：数据集中的单个数据点（x, y）
- 训练集：用于训练模型的数据集
- 测试集：用于评估模型的数据集
- 损失函数：衡量模型预测结果与实际结果之间差异的函数
- 优化器：用于调整模型参数以最小化损失函数的算法
